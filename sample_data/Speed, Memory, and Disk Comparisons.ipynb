{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652c54c0",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Disk Comparisons\n",
    "\n",
    "In this notebook, we'll offer some rough comparisons of the computational performance implications of ESGPT vs. other competing pipelines. We'll focus these comparisons on several metrics:\n",
    "  1. The time, runtime memory, and final disk space required to construct, pre-process, and store an ESGPT dataset relative to other pipelines, where applicable.\n",
    "  2. The initialization time, iteration speed, and GPU memory costs for producing batches of data within the ESGPT framework vs. other systems.\n",
    "  \n",
    "In particular, we'll compare (or justify why they are inappropriate comparators) against the following pipelines:\n",
    "  1. TemporAI\n",
    "  2. OMOP-Learn\n",
    "  3. FIDDLE\n",
    "  4. MIMIC-Extract\n",
    "  \n",
    "We'll make these comparisons leveraging the synthetic data distributed with ESGPT's sample tutorial, but this code can also be ported to any other dataset to run these profiles locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23533dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from humanize import naturalsize, naturaldelta\n",
    "from pathlib import Path\n",
    "from sparklines import sparklines\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "from EventStream.data.config import PytorchDatasetConfig\n",
    "from EventStream.data.types import PytorchBatch\n",
    "from EventStream.data.pytorch_dataset import PytorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87fd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6314e23",
   "metadata": {},
   "source": [
    "First, let's check and see how much disk space the dataset uses, and in what components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset takes up 31.2 MB on disk, which includes:\n",
      "  * 19.5 MB for the core dataset.\n",
      "  * 11.7 MB for the deep-learning representation dataframes.\n"
     ]
    }
   ],
   "source": [
    "total_dataset_size = sum(f.stat().st_size for f in dataset_dir.glob('**/*') if f.is_file())\n",
    "DL_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"DL_reps\").glob('**/*') if f.is_file())\n",
    "just_dataset_size = total_dataset_size - DL_reps_size\n",
    "\n",
    "if (dataset_dir / \"flat_reps\").is_dir():\n",
    "    flat_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"flat_reps\").glob('**/*') if f.is_file())\n",
    "    just_dataset_size -= flat_reps_size\n",
    "    flat_reps_lines = [f\"  * {naturalsize(flat_reps_size)} for the flat representation dataframes.\"]\n",
    "else:\n",
    "    flat_reps_lines = []\n",
    "\n",
    "lines = [\n",
    "    f\"The total dataset takes up {naturalsize(total_dataset_size)} on disk, which includes:\",\n",
    "    f\"  * {naturalsize(just_dataset_size)} for the core dataset.\",\n",
    "    f\"  * {naturalsize(DL_reps_size)} for the deep-learning representation dataframes.\",\n",
    "] + flat_reps_lines\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d866",
   "metadata": {},
   "source": [
    "First, we'll note that loading a dataset doesn't require much of either resource. This is because the data is loaded lazily, so complex dataframe elements aren't loaded until they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 348.25 MiB, increment: 2.13 MiB\n",
      "CPU times: user 123 ms, sys: 22.3 ms, total: 145 ms\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92467145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n",
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n",
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n",
      "peak memory: 510.11 MiB, increment: 161.67 MiB\n",
      "CPU times: user 270 ms, sys: 85.8 ms, total: 356 ms\n",
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "s_df = ESD.subjects_df\n",
    "e_df = ESD.events_df\n",
    "dm_df = ESD.dynamic_measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee9d9d",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Stats\n",
    "Now let's load a pytorch dataset and examine iteration speed and GPU memory cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr: list[float], strify: Callable[float, str] = naturalsize) -> str:\n",
    "    mean, std, mn, mx = np.mean(arr), np.std(arr), np.min(arr), np.max(arr)\n",
    "    simple_summ = f\"{strify(mean)} Â± {strify(std)} ({strify(mn)}-{strify(mx)})\"\n",
    "    \n",
    "    if len(arr) < 25: return simple_summ\n",
    "    \n",
    "    hist_vals, hist_bins = np.histogram(arr)\n",
    "    lines = [simple_summ, \"Histogram:\"]\n",
    "    sparkline = sparklines(hist_vals)\n",
    "    \n",
    "    lines.extend(sparkline)\n",
    "    left_end = strify(hist_bins[0])\n",
    "    right_end = strify(hist_bins[1])\n",
    "    W = len(sparkline[0]) - len(left_end) - len(right_end)\n",
    "    \n",
    "    if W > 0:\n",
    "        lines.append(f\"{left_end}{'-'*W}{right_end}\")\n",
    "    else:\n",
    "        lines.append(f\"o {left_end} (left endpoint)\")\n",
    "        lines.append(f\"{'-'*(len(sparkline[0])-1)}o {right_end} (right endpoint)\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def summarize_times(arr: list[float, timedelta]):\n",
    "    as_seconds = [x / timedelta(seconds=1) for x in arr]\n",
    "    return summarize(as_seconds, strify=lambda x: str(timedelta(seconds=x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d64100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_batch_iteration_speed_and_cost(\n",
    "    batch_size: int,\n",
    "    pyd: Dataset,\n",
    "    n_iter_samples: int = 30,\n",
    "    collate_fn: Callable | None = None,\n",
    "    num_workers: int | None = None,\n",
    "):\n",
    "    def make_dataloader():\n",
    "        dataloader_kwargs = {'dataset': pyd, 'batch_size': batch_size, 'shuffle': True}\n",
    "        if collate_fn is not None:\n",
    "            dataloader_kwargs['collate_fn'] = collate_fn\n",
    "        if num_workers is not None:\n",
    "            dataloader_kwargs['num_workers'] = num_workers\n",
    "        return DataLoader(**dataloader_kwargs)\n",
    "\n",
    "    dataloader = make_dataloader()\n",
    "    batch_sizes = defaultdict(list)\n",
    "    total_sizes = []\n",
    "    for batch in tqdm(dataloader, leave=False):\n",
    "        total_size = 0\n",
    "        for k, v in batch.items():\n",
    "            if v is None: continue\n",
    "            el_size = v.element_size() * v.nelement()\n",
    "            batch_sizes[k].append(el_size)\n",
    "            total_size += el_size\n",
    "        total_sizes.append(total_size)\n",
    "\n",
    "    batch_iteration_times = []\n",
    "    for samp in tqdm(list(range(n_iter_samples)), leave=False, desc=\"Sampling Dataloader Iteration Speed\"):\n",
    "        dataloader = make_dataloader()\n",
    "        st = datetime.now()\n",
    "        for batch in tqdm(dataloader, leave=False, desc=\"Sampling Batch\"):\n",
    "            pass\n",
    "        batch_iteration_times.append((datetime.now() - st) / len(dataloader))\n",
    "\n",
    "    print(\n",
    "        f\"Iterating through an entire dataloader of {len(dataloader)} batches of size {batch_size} \"\n",
    "        f\"took the following time per batch:\\n{summarize_times(batch_iteration_times)}\\n\\n\"\n",
    "        f\"Total batch size:\\n{summarize(total_sizes)}\"\n",
    "    )\n",
    "    for k, v in batch_sizes.items():\n",
    "        print(f\"  Size of {k}:\\n    {summarize(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d054bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 831.51 MiB, increment: 320.94 MiB\n",
      "CPU times: user 2.09 s, sys: 181 ms, total: 2.27 s\n",
      "Wall time: 2.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "pyd_config = PytorchDatasetConfig(\n",
    "    save_dir=ESD.config.save_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "pyd = PytorchDataset(config=pyd_config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c71a555",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 5 batches of size 16 took the following time per batch:\n",
      "0:00:01.065503 Â± 0:00:00.069628 (0:00:00.999924-0:00:01.327426)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 0:00:00.999924 (left endpoint)\n",
      "---------o 0:00:01.032674 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "7.7 MB Â± 958.5 kB (6.6 MB-9.4 MB)\n",
      "  Size of event_mask:\n",
      "    16.4 kB Â± 0 Bytes (16.4 kB-16.4 kB)\n",
      "  Size of time_delta:\n",
      "    65.5 kB Â± 0 Bytes (65.5 kB-65.5 kB)\n",
      "  Size of static_indices:\n",
      "    128 Bytes Â± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of static_measurement_indices:\n",
      "    128 Bytes Â± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of dynamic_indices:\n",
      "    2.9 MB Â± 365.1 kB (2.5 MB-3.5 MB)\n",
      "  Size of dynamic_measurement_indices:\n",
      "    2.9 MB Â± 365.1 kB (2.5 MB-3.5 MB)\n",
      "  Size of dynamic_values:\n",
      "    1.5 MB Â± 182.6 kB (1.2 MB-1.8 MB)\n",
      "  Size of dynamic_values_mask:\n",
      "    363.7 kB Â± 45.6 kB (311.3 kB-442.4 kB)\n"
     ]
    }
   ],
   "source": [
    "batch_size=16\n",
    "profile_batch_iteration_speed_and_cost(\n",
    "    batch_size=batch_size, pyd=pyd, n_iter_samples=30, collate_fn=pyd.collate,\n",
    "    num_workers=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd999ce",
   "metadata": {},
   "source": [
    "## Other Pipelines\n",
    "### TemporAI Format\n",
    "First, we'll compare against [TemporAI](https://www.temporai.vanderschaar-lab.com/), a recent library that provides a modular set of pre-built plugins for processing temporal EHR data. \n",
    "\n",
    "Their data representation differs from ESGPT's in that it is a _wide_ representation, vs. our _long_ representation at a per-event level. More specifically, whereas ESGPT data structures store the measurements observed per event in a nested list per event, TemporAI data structures pivot that structure and store all measurements (observed or unobserved) in separate columns for each event, stored as a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a013487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d978222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_to_temporai(ESD: Dataset) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Converts an ESD data format into a TemporAI dataset format.\"\"\"\n",
    "\n",
    "    static_df = (\n",
    "        ESD.subjects_df\n",
    "        .filter(pl.col('subject_id').is_in(list(ESD.split_subjects['train'])))\n",
    "        .select(\n",
    "            'subject_id',\n",
    "            *[pl.col(c) for c, cfg in ESD.measurement_configs.items() if cfg.temporality == 'static']\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(\"subject_id\")\n",
    "    )\n",
    "    \n",
    "    # For the time-series dataframe, as they need only one row per subject ID, timestamp, we need to use the wide\n",
    "    # format of the flat representation. \n",
    "    \n",
    "    flat_reps_dir = ESD.config.save_dir / \"flat_reps\" / \"raw\"\n",
    "    if not flat_reps_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Must have pre-cached flat representations at {flat_reps_dir}!\")\n",
    "        \n",
    "    time_series_df = (\n",
    "        pl.scan_parquet(flat_reps_dir / \"train\" / \"*.parquet\")\n",
    "        .select(\"subject_id\", \"timestamp\", cs.starts_with(\"dynamic\"))\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "        .set_index([\"subject_id\", \"timestamp\"])\n",
    "    )\n",
    "    \n",
    "    return static_df, time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67048d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening Splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1993.54 MiB, increment: 1174.56 MiB\n",
      "CPU times: user 21.8 s, sys: 3.39 s, total: 25.2 s\n",
      "Wall time: 7.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# We need to convert to a flat format prior to getting temporai representations.\n",
    "# The performance #s here are not reliable as these files may be already generated.\n",
    "ESD.cache_flat_representation(\n",
    "    subjects_per_output_file=None,\n",
    "    feature_inclusion_frequency=None,\n",
    "    do_overwrite=False,\n",
    "    do_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bde3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2063.73 MiB, increment: 965.48 MiB\n",
      "CPU times: user 2.2 s, sys: 1.81 s, total: 4.02 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static, temporai_ts = ESD_to_temporai(ESD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f6cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporAI uses two dataframes, a static dataframe of shape (100, 1) and a time series dataframe of shape (530742, 160).\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"TemporAI uses two dataframes, a static dataframe of shape {temporai_static.shape} \"\n",
    "    f\"and a time series dataframe of shape {temporai_ts.shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7346bbe",
   "metadata": {},
   "source": [
    "Let's save these dataframes to disk, so we can inspect their disk cost and the memory cost to re-load them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41127e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compressed data takes up 23.9 MB on disk.\n",
      "The uncompressed data takes up 26.0 MB on disk (this is a good approximation of memory cost as it is uncompressed).\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"./speed_comparisons/temporai/compressed\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts.to_parquet(save_dir / \"ts.parquet\")\n",
    "\n",
    "uncompressed_save_dir = Path(\"./speed_comparisons/temporai/uncompressed\")\n",
    "uncompressed_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(uncompressed_save_dir / \"static.parquet\", compression=None)\n",
    "temporai_ts.to_parquet(uncompressed_save_dir / \"ts.parquet\", compression=None)\n",
    "\n",
    "compressed_temporai_size = sum(f.stat().st_size for f in save_dir.glob('**/*') if f.is_file())\n",
    "uncompressed_temporai_size = sum(f.stat().st_size for f in uncompressed_save_dir.glob('**/*') if f.is_file())\n",
    "\n",
    "print(\n",
    "    f\"The compressed data takes up {naturalsize(compressed_temporai_size)} on disk.\\n\"\n",
    "    f\"The uncompressed data takes up {naturalsize(uncompressed_temporai_size)} on disk \"\n",
    "    \"(this is a good approximation of memory cost as it is uncompressed).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a5fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2603.92 MiB, increment: 916.96 MiB\n",
      "CPU times: user 1.21 s, sys: 806 ms, total: 2.01 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static = pd.read_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts = pd.read_parquet(save_dir / \"ts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13a4a",
   "metadata": {},
   "source": [
    "TemporAI generally converts their timeseries data into a dense, 3D matrix across samples, timepoints, and features. For use in ML pipelines, this is then generally iterated through directly via simple numpy iteration. \n",
    "\n",
    "For example: \n",
    "  * Datasets are converted to 3D views here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/plugins/prediction/one_off/classification/__init__.py#L59 and https://github.com/vanderschaarlab/temporai/blob/67ebd74dc24728163d9aec37f1771a83fc3346e2/src/tempor/data/utils.py#L49\n",
    "  * Iteration through numpy arrays happens here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/models/ddh.py#L155\n",
    "  \n",
    "Though a full comparison warrants use of their library (and will further depend on the exact model used (as each has different strategies for processing data), we can simulate that approach here quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8bb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_categories(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[c]):\n",
    "            df[c] = df[c].cat.codes\n",
    "    return df\n",
    "\n",
    "def to_3D_arr(df: pd.DataFrame, max_timesteps: int | None = None) -> np.ndarray:\n",
    "    df = no_categories(df)\n",
    "    samples = set(df.index.get_level_values(0))\n",
    "    num_samples = len(samples)\n",
    "    num_features = len(df.columns)\n",
    "    num_timesteps_per_sample = df.groupby(level=0).size()\n",
    "    max_actual_timesteps = num_timesteps_per_sample.max()\n",
    "    max_timesteps = max_actual_timesteps if max_timesteps is None else max_timesteps\n",
    "    array = np.full(shape=(num_samples, max_timesteps, num_features), fill_value=np.NaN)\n",
    "    for i_sample, idx_sample in enumerate(samples):\n",
    "        set_vals = df.loc[idx_sample, :, :].to_numpy()[:max_timesteps, :]  # pyright: ignore\n",
    "        if i_sample == 0:\n",
    "            array = array.astype(set_vals.dtype)  # Need to cast to the type matching source data.\n",
    "        array[i_sample, : num_timesteps_per_sample[idx_sample], :] = set_vals  # pyright: ignore\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367b17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTemporAIStyleDataset(Dataset):\n",
    "    def __init__(self, static: np.ndarray, ts: np.ndarray):\n",
    "        self.static = static\n",
    "        self.ts = ts\n",
    "        \n",
    "    def __len__(self) -> int: return self.ts.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict[str, torch.Tensor]:\n",
    "        return {'static': torch.Tensor(self.static[idx]), 'ts': torch.Tensor(self.ts[idx])}\n",
    "    \n",
    "def profile_temporai_dataset(\n",
    "    temporai_static, temporai_ts, batch_size: int = 16,\n",
    "    n_iter_samples: int = 30,\n",
    "    max_seq_len: int = 32,\n",
    "):\n",
    "    static_as_np = np.nan_to_num(no_categories(temporai_static).to_numpy(), nan=0)\n",
    "    ts_as_np = np.nan_to_num(to_3D_arr(temporai_ts, max_timesteps=max_seq_len), nan=0)\n",
    "    print(\n",
    "        f\"Yielded a static NP array of shape {static_as_np.shape} and a TS NP array \"\n",
    "        f\"of shape {ts_as_np.shape}.\"\n",
    "    )\n",
    "    temporai_pyd = SimpleTemporAIStyleDataset(static_as_np, ts_as_np)\n",
    "\n",
    "    profile_batch_iteration_speed_and_cost(\n",
    "        batch_size=batch_size, pyd=temporai_pyd, n_iter_samples=n_iter_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba4786df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielded a static NP array of shape (100, 1) and a TS NP array of shape (100, 1024, 160).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 7 batches of size 16 took the following time per batch:\n",
      "0:00:00.008704 Â± 0:00:00.002816 (0:00:00.006541-0:00:00.022163)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 0:00:00.006541 (left endpoint)\n",
      "---------o 0:00:00.008103 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "9.4 MB Â± 2.8 MB (2.6 MB-10.5 MB)\n",
      "  Size of static:\n",
      "    57 Bytes Â± 16 Bytes (16 Bytes-64 Bytes)\n",
      "  Size of ts:\n",
      "    9.4 MB Â± 2.8 MB (2.6 MB-10.5 MB)\n",
      "peak memory: 2325.00 MiB, increment: 273.90 MiB\n",
      "CPU times: user 8.6 s, sys: 822 ms, total: 9.42 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_temporai_dataset(temporai_static, temporai_ts, batch_size=16, n_iter_samples=30, max_seq_len=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906d5aa",
   "metadata": {},
   "source": [
    "As we can see, the strategy of featurizing and batching used in TemporAI results (on this synthetic dataset) in a significantly faster iteration speed and a marginally lower memory cost than does the strategy used in ESGPT (all formats are mean Â± standard deviation (min - max)\n",
    "\n",
    "TemporAI Speed: `0:00:00.007319 Â± 0:00:00.001439 (0:00:00.006224-0:00:00.012835)`  \n",
    "ESGPT Speed:    `0:00:00.734263 Â± 0:00:00.078552 (0:00:00.586128-0:00:00.871448)`\n",
    "\n",
    "TemporAI Memory: `9.4 MB Â± 2.8 MB (2.6 MB-10.5 MB)`  \n",
    "ESGPT Memory:    `8.3 MB Â± 1.1 MB (6.6 MB-9.7 MB)`\n",
    "\n",
    "In table form (using chatGPT for conversions, so may need to be double checked), where \"Delta\" means what % of TemporAI's resource cost does ESGPT _save_ (higher is better), we get the following:\n",
    "|                      | TemporAI              | ESGPT                 | Delta (%)  |\n",
    "|----------------------|-----------------------|-----------------------|------------|\n",
    "| **Iteration time / batch (ms)** | 7.32 Â± 1.44 (6.22 - 12.8) | 734 Â± 78.6 (586 - 871) | -9943%           |\n",
    "| **Memory (MB)**      | 9.4 Â± 2.8 (2.6 - 10.5) | 8.3 Â± 1.1 (6.6 - 9.7) | 11.7%             |\n",
    "\n",
    "There are some biases in this format, on both sides:\n",
    "  1. ESGPT samples different subsequences per item iteration, whereas TemporAI is limited to only using the first max subsequence samples. \n",
    "  2. This dataset has relatively few measurements, which will reduce the memory disparity between the two formats (this bias favors TemporAI).\n",
    "  3. The strategy of flattening this dataset may induce too much memory overhead, as if multiple measurements are not common within an event, it will have extra columns that TemporAI does not need. Conversely, it may reduce a significant amount of data, as if there are many measurements than a simple count, sum, sum_sqd, min, and max representation will not fully capture the data, thereby reducing the burden on TemporAI. (This bias could favor either).\n",
    "  \n",
    "#### On MIMIC-IV\n",
    "Ultimately, these numbers will only be truly reasonable when compared on real data. To do so, we can use MIMIC-IV. While the full numbers for the MIMIC-IV dataset can be found in that example's repository, here we summarize the table obtained when running on that dataset with an otherwise nearly identical setup:\n",
    "\n",
    "|                      | TemporAI             | ESGPT                | Improvement (%)  |\n",
    "|----------------------|----------------------|----------------------|-------------------|\n",
    "| **Iteration time / batch (ms)**       | 597 Â± 20.6 (571 - 624) | 416 Â± 4.16 (410 - 423) | 30.3%            |\n",
    "| **Memory (MB)**      | 877 Â± 95.5 (507 - 902) | 67.2 Â± 35.0 (18.3 - 411) | 92.3%           |\n",
    "\n",
    "We can see that the numbers here are very different, favoring ESGPT dramatically more than we saw on sample data. The reasons for this are primarily due to how memory intensive TemporAI's coding system is; whereas ESGPT only stores data-elements in a batch if those elements were actually observed in the record, TemporAI's flat encoding means that batches must have a memory cost that scales with the total vocabulary size (and overall maximum sequence length) of the data. This higher cost impacts both speed and memory, though memory is the clear focus for ESGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22b991",
   "metadata": {},
   "source": [
    "### omop-learn\n",
    "\n",
    "Next, we'll attempt to compare against [omop-Learn](https://github.com/clinicalml/omop-learn/tree/master). omop-learn uses a similar storage model to ESGPT (a _long_ format) but it does not include the storage of numerical values, only codes, and it similarly does not allow for randomly sampled sub-sequences per dataset item. See [here](https://github.com/clinicalml/omop-learn/blob/a33440af2b9f1342e0c16106acf93131b9369441/src/omop_learn/torch/data.py) for more details. To simulate this, we'll first convert the ESGPT data into a pre-tokenized OMOP-learn representation, store in the desired format (JSON), then re-load it and iterate through it in the same manner as omop-learn does. As static data are not separately encoded in the omop-learn dataset format, we'll omit that as well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f6b09467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import json, shutil\n",
    "REFTIME = pd.Timestamp(\"1900-01-01\")\n",
    "\n",
    "def to_unixtime(str_time_array):\n",
    "    unix_times = (pd.to_datetime(str_time_array) - REFTIME) // pd.Timedelta(\"1d\")\n",
    "    return unix_times\n",
    "\n",
    "\n",
    "def from_unixtime(unix_time_array):\n",
    "    datetimes = [pd.to_datetime(t * pd.Timedelta(\"1d\") + REFTIME) for t in unix_time_array]\n",
    "    return datetimes\n",
    "\n",
    "def ESD_row_to_omoplearn_row(row: dict[str, Any], unified_vocab: dict[int, str]) -> dict[str, Any]:\n",
    "    example = {}\n",
    "    \n",
    "    # Dates\n",
    "    example['dates'] = [str(row['start_time'] + timedelta(minutes=t)) for t in row['time']]\n",
    "    \n",
    "    # Visits\n",
    "    example['visits'] = [\n",
    "        [unified_vocab[idx] for idx in indices] for indices in row['dynamic_indices']\n",
    "    ]\n",
    "    \n",
    "    example['tok_visits'] = row['dynamic_indices']\n",
    "    \n",
    "    assert len(example['dates']) == len(example['visits'])\n",
    "    assert len(example['dates']) == len(example['tok_visits'])\n",
    "    \n",
    "    example['y'] = 0\n",
    "    \n",
    "    return example\n",
    "\n",
    "def ESD_to_omoplearn(ESD: Dataset) -> list[dict[str, Any]]:\n",
    "    unified_vocab = {}\n",
    "    for m, idxmap in ESD.unified_vocabulary_idxmap.items():\n",
    "        for k, v in idxmap.items():\n",
    "            unified_vocab[v] = f\"{m}/{k}\"\n",
    "    \n",
    "    omop_file_dir = Path(\"./omop_reps\")\n",
    "    if omop_file_dir.is_dir():\n",
    "        shutil.rmtree(omop_file_dir)\n",
    "\n",
    "    omop_file_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    for sp in tqdm(list(ESD.split_subjects.keys()), desc=\"JSONifying Splits\", leave=False):\n",
    "        omop_sp_dir = omop_file_dir / sp\n",
    "        omop_sp_dir.mkdir(exist_ok=True, parents=True)\n",
    "        for fp in tqdm(\n",
    "            list((ESD.config.save_dir / \"DL_reps\").glob(f\"{sp}_*.parquet\")), desc=\"File\", leave=False\n",
    "        ):\n",
    "            DL_reps_df = pl.read_parquet(fp)\n",
    "            cols = DL_reps_df.columns\n",
    "            rows = DL_reps_df.rows()\n",
    "            \n",
    "            for row in rows:\n",
    "                row_as_dict = {col: val for col, val in zip(cols, row)}\n",
    "                example = ESD_row_to_omoplearn_row(row_as_dict, unified_vocab)\n",
    "                with open(omop_sp_dir / \"data.json\", \"a\") as f:\n",
    "                    json.dump(example, f)\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e009fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JSONifying Splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "File:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "File:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "File:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2246.19 MiB, increment: 42.89 MiB\n",
      "CPU times: user 7.5 s, sys: 477 ms, total: 7.98 s\n",
      "Wall time: 7.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD_to_omoplearn(ESD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef0897b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The JSON dumped dataset takes up 66.2 MB on disk.\n"
     ]
    }
   ],
   "source": [
    "JSON_dataset_size = sum(f.stat().st_size for f in Path(\"./omop_reps\").glob('**/*') if f.is_file())\n",
    "print(f\"The JSON dumped dataset takes up {naturalsize(JSON_dataset_size)} on disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2f312772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightly adapted from https://github.com/clinicalml/omop-learn/blob/a33440af2b9f1342e0c16106acf93131b9369441/src/omop_learn/torch/data.py\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class OMOPDatasetTorch(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        omop_dataset_file,\n",
    "        max_num_visits=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.items = {}\n",
    "        self.visit_sequences = []  # patient x (# visits for patient) lists w/ concepts expressed\n",
    "        self.time_sequences = []  # patient x (# visits for patient) times of visits\n",
    "        self.visit_sizes = []  # patient x (# visits for patient) # concepts in each visit\n",
    "        self.outcomes = []  # patient--outcome for each patient\n",
    "        self.tok_visit_sequences = None\n",
    "        self.tokenizer = None\n",
    "        self.max_num_visits = max_num_visits  # if set, truncate to most recent\n",
    "        self._load_json(omop_dataset_file, False)\n",
    "\n",
    "\n",
    "    def _load_json(self, path_to_json, tokenize_on_load):\n",
    "        # read once to build concept set\n",
    "        # (and load visits if tokenize_on_load=False)\n",
    "        concept_set = set()\n",
    "        concept_counts = Counter()\n",
    "        concept_counts_by_year = Counter()\n",
    "        years = set()\n",
    "        max_num_visits = 0\n",
    "        skipped = 0\n",
    "        with open(path_to_json) as json_fh:\n",
    "            for i, line in enumerate(json_fh.readlines()):\n",
    "                example = self._process_line(line)\n",
    "                max_num_visits = max(max_num_visits, len(example['visits']))\n",
    "\n",
    "                for time, visit in zip(example['unix_times'], example['visits']):\n",
    "                    for concept in visit:\n",
    "                        concept_set.add(concept)\n",
    "\n",
    "                if len(example['visits']) == 0:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                if i == 0:\n",
    "                    for key, value in example.items():\n",
    "                        self.items[key] = [value]\n",
    "                else:\n",
    "                    # correctly gives error when key is not found\n",
    "                    # already; all items need to have exactly the same\n",
    "                    # set of keys.\n",
    "                    for key,value in example.items():\n",
    "                        self.items[key].append(value)\n",
    "\n",
    "        print(f\"Skipped {skipped} patients for empty visit lists\")\n",
    "        if not self.max_num_visits:\n",
    "            self.max_num_visits = max_num_visits\n",
    "\n",
    "#         if not self.tokenizer:\n",
    "#             self.tokenizer = ConceptTokenizer(concept_set)\n",
    "#             print(\"built tokenizer\")\n",
    "\n",
    "#         # read again to build tokenized visits\n",
    "#         if tokenize_on_load:\n",
    "#             self.items['tok_visits'] = []\n",
    "#             with open(path_to_json, \"r\") as json_fh:\n",
    "#                 for i, line in enumerate(json_fh.readlines()):\n",
    "#                     example = self._process_line(line)\n",
    "#                     tok_visit_list = []\n",
    "#                     for visit in example['visits']:\n",
    "#                         tok_visit = self.tokenizer.concepts_to_ids(visit)\n",
    "#                         tok_visit_list.append(tok_visit)\n",
    "#                     if len(tok_visit_list) > 0:\n",
    "#                         self.items['tok_visits'].append(tok_visit_list)\n",
    "\n",
    "        self.outcomes = torch.LongTensor(self.items['y'])\n",
    "        self.one_fraction = self.outcomes.sum() / len(self.outcomes)\n",
    "        self.one_odds = self.one_fraction / (1 - self.one_fraction)\n",
    "\n",
    "    def _process_line(self, line):\n",
    "        example = json.loads(line)\n",
    "        dates = example['dates']\n",
    "        unix_times = to_unixtime(dates)\n",
    "        example['unix_times'] = unix_times\n",
    "\n",
    "        # make sure visits are sorted by date\n",
    "        # This actually contains a minor error correction in omop-learn; namely, that pre-tokenized visits may not\n",
    "        # have been properly sorted.\n",
    "        sorted_visits = [v for d,v in sorted(zip(example['unix_times'], example['visits']))]\n",
    "        if 'tok_visits' in example:\n",
    "            sorted_tok_visits = [t for d,t in sorted(zip(example['unix_times'], example['tok_visits']))]\n",
    "            example['tok_visits'] = sorted_tok_visits\n",
    "        example['visits'] = sorted_visits\n",
    "        example['unix_times'] = sorted(example['unix_times'])\n",
    "        example['dates'] = sorted(example['dates'])\n",
    "        \n",
    "        assert len(example['visits']) == len(example['unix_times'])\n",
    "        assert len(example['tok_visits']) == len(example['unix_times'])\n",
    "\n",
    "        return example\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = {k : v[idx] for k,v in self.items.items()}\n",
    "        times = torch.LongTensor(example['unix_times'])\n",
    "        visits = example['visits'] if 'tok_visits' not in example else example['tok_visits']\n",
    "        assert len(times) == len(visits)\n",
    "\n",
    "        # trim before tokenizing\n",
    "        visits = visits[-self.max_num_visits :]\n",
    "        times = times[-self.max_num_visits :]\n",
    "\n",
    "        if 'tok_visits' not in example:\n",
    "            raise NotImplementedError(f\"Must be pre-tokenized!\")\n",
    "#             tok_visits = []\n",
    "#             for visit in example['visits']:\n",
    "#                 tok_visits.append(self.tokenizer.concepts_to_ids(visit))\n",
    "#             visits = tok_visits\n",
    "\n",
    "        example['visits'] = visits\n",
    "        # Another small bug correction\n",
    "        example['unix_times'] = times\n",
    "\n",
    "        visit_sizes = torch.LongTensor([len(v) for v in visits])\n",
    "        outcome = self.outcomes[idx]\n",
    "        nvisits = len(visits)\n",
    "\n",
    "        return example\n",
    "\n",
    "    # pads a batch to largest # of visits / patient in the batch\n",
    "    # and largest # of concepts / visit along concept dim.\n",
    "    def collate(self, batch):\n",
    "        # first group along dict keys\n",
    "        batch_collated = {}\n",
    "        for k in batch[0].keys():\n",
    "            batch_collated[k] = [b[k] for b in batch]\n",
    "\n",
    "        keys = list(batch_collated.keys())\n",
    "        N = len(batch_collated['y'])\n",
    "\n",
    "        # each patient is a list of visits\n",
    "        max_num_visits = max([len(v) for v in batch_collated['visits']])\n",
    "        max_num_concepts = max(l for p in range(N) for l in [len(v) for v in batch_collated['visits'][p]])\n",
    "\n",
    "        concept_tensor = torch.full(\n",
    "            (N, max_num_visits, max_num_concepts),\n",
    "            0,\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        times_tensor = torch.full((N, max_num_visits), -1, dtype=torch.long)\n",
    "        batch = batch_collated\n",
    "\n",
    "        lengths = torch.zeros(N)\n",
    "        for i, visit_list in enumerate(batch['visits']):\n",
    "            assert len(visit_list) == len(batch['unix_times'][i]), (\n",
    "                f\"Visits don't match! Got {len(visit_list)} visits and {len(batch['unix_times'][i])} times \"\n",
    "                f\"for batch element {i}\"\n",
    "            )\n",
    "            num_visits = len(visit_list)  # visits of this patient we are including\n",
    "            lengths[i] = num_visits\n",
    "            for j, visit in enumerate(visit_list):\n",
    "                visit_size = len(batch['visits'][i][j])\n",
    "                assert(visit_size == len(visit))\n",
    "                concept_tensor[i, j, : visit_size] = torch.Tensor(visit)\n",
    "            times_tensor[i, :num_visits] = torch.Tensor(batch['unix_times'][i])\n",
    "        batch.pop(\"unix_times\")\n",
    "        batch.pop(\"dates\")\n",
    "        # Another small correction:\n",
    "        batch.pop(\"tok_visits\")\n",
    "        batch[\"visits\"] = concept_tensor\n",
    "        batch[\"times\"] = times_tensor\n",
    "        batch[\"lengths\"] = lengths\n",
    "        for k,v in batch.items():\n",
    "            if not isinstance(v, torch.Tensor):\n",
    "                batch[k] = torch.tensor(v)\n",
    "        return batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e6212ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 0 patients for empty visit lists\n",
      "peak memory: 2218.59 MiB, increment: 123.47 MiB\n",
      "CPU times: user 3.37 s, sys: 425 ms, total: 3.8 s\n",
      "Wall time: 3.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "ODT = OMOPDatasetTorch(\"./omop_reps/train/data.json\", max_num_visits=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "87bf36f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 5 batches of size 16 took the following time per batch:\n",
      "0:00:00.364018 Â± 0:00:00.049781 (0:00:00.190901-0:00:00.470015)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 0:00:00.190901 (left endpoint)\n",
      "---------o 0:00:00.218812 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "3.5 MB Â± 365.1 kB (3.0 MB-3.9 MB)\n",
      "  Size of visits:\n",
      "    3.4 MB Â± 365.1 kB (2.9 MB-3.8 MB)\n",
      "  Size of y:\n",
      "    128 Bytes Â± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of times:\n",
      "    131.1 kB Â± 0 Bytes (131.1 kB-131.1 kB)\n",
      "  Size of lengths:\n",
      "    64 Bytes Â± 0 Bytes (64 Bytes-64 Bytes)\n",
      "peak memory: 2219.23 MiB, increment: 0.63 MiB\n",
      "CPU times: user 2min 27s, sys: 1.35 s, total: 2min 29s\n",
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_batch_iteration_speed_and_cost(\n",
    "    batch_size=16, pyd=ODT, n_iter_samples=30, collate_fn=ODT.collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42197d",
   "metadata": {},
   "source": [
    "We can't do a true apples to apples comparison in terms of speed, here, as there is less data being processed in the OMOP-learn setting, but in terms of Memory we can break down the cost for just the dynamic indices components of the ESGPT batch, and find that it equates to roughly 3MB per batch, just under what OMOP-learn requires. However, we do need to test this on real data to see the true comparison. Additionally, note that this is testing only the pre-tokenized version of OMOP-learn, which is faster than the on-the-fly tokenizing mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0925532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
