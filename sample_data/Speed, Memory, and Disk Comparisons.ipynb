{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652c54c0",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Disk Comparisons\n",
    "\n",
    "In this notebook, we'll offer some rough comparisons of the computational performance implications of ESGPT vs. other competing pipelines. We'll focus these comparisons on several metrics:\n",
    "  1. The time, runtime memory, and final disk space required to construct, pre-process, and store an ESGPT dataset relative to other pipelines, where applicable.\n",
    "  2. The initialization time, iteration speed, and GPU memory costs for producing batches of data within the ESGPT framework vs. other systems.\n",
    "  \n",
    "In particular, we'll compare (or justify why they are inappropriate comparators) against the following pipelines:\n",
    "  1. TemporAI\n",
    "  2. OMOP-Learn\n",
    "  3. FIDDLE\n",
    "  4. MIMIC-Extract\n",
    "  \n",
    "We'll make these comparisons leveraging the synthetic data distributed with ESGPT's sample tutorial, but this code can also be ported to any other dataset to run these profiles locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23533dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from humanize import naturalsize, naturaldelta\n",
    "from pathlib import Path\n",
    "from sparklines import sparklines\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "from EventStream.data.config import PytorchDatasetConfig\n",
    "from EventStream.data.types import PytorchBatch\n",
    "from EventStream.data.pytorch_dataset import PytorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87fd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6314e23",
   "metadata": {},
   "source": [
    "First, let's check and see how much disk space the dataset uses, and in what components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset takes up 164.5 MB on disk, which includes:\n",
      "  * 19.5 MB for the core dataset.\n",
      "  * 11.7 MB for the deep-learning representation dataframes.\n",
      "  * 133.2 MB for the flat representation dataframes.\n"
     ]
    }
   ],
   "source": [
    "total_dataset_size = sum(f.stat().st_size for f in dataset_dir.glob('**/*') if f.is_file())\n",
    "DL_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"DL_reps\").glob('**/*') if f.is_file())\n",
    "just_dataset_size = total_dataset_size - DL_reps_size\n",
    "\n",
    "if (dataset_dir / \"flat_reps\").is_dir():\n",
    "    flat_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"flat_reps\").glob('**/*') if f.is_file())\n",
    "    just_dataset_size -= flat_reps_size\n",
    "    flat_reps_lines = [f\"  * {naturalsize(flat_reps_size)} for the flat representation dataframes.\"]\n",
    "else:\n",
    "    flat_reps_lines = []\n",
    "\n",
    "lines = [\n",
    "    f\"The total dataset takes up {naturalsize(total_dataset_size)} on disk, which includes:\",\n",
    "    f\"  * {naturalsize(just_dataset_size)} for the core dataset.\",\n",
    "    f\"  * {naturalsize(DL_reps_size)} for the deep-learning representation dataframes.\",\n",
    "] + flat_reps_lines\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d866",
   "metadata": {},
   "source": [
    "First, we'll note that loading a dataset doesn't require much of either resource. This is because the data is loaded lazily, so complex dataframe elements aren't loaded until they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 347.58 MiB, increment: 1.76 MiB\n",
      "CPU times: user 124 ms, sys: 29.3 ms, total: 153 ms\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92467145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n",
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n",
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n",
      "peak memory: 510.47 MiB, increment: 162.46 MiB\n",
      "CPU times: user 398 ms, sys: 103 ms, total: 501 ms\n",
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "s_df = ESD.subjects_df\n",
    "e_df = ESD.events_df\n",
    "dm_df = ESD.dynamic_measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee9d9d",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Stats\n",
    "Now let's load a pytorch dataset and examine iteration speed and GPU memory cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr: list[float], strify: Callable[float, str] = naturalsize) -> str:\n",
    "    mean, std, mn, mx = np.mean(arr), np.std(arr), np.min(arr), np.max(arr)\n",
    "    simple_summ = f\"{strify(mean)} ± {strify(std)} ({strify(mn)}-{strify(mx)})\"\n",
    "    \n",
    "    if len(arr) < 25: return simple_summ\n",
    "    \n",
    "    hist_vals, hist_bins = np.histogram(arr)\n",
    "    lines = [simple_summ, \"Histogram:\"]\n",
    "    sparkline = sparklines(hist_vals)\n",
    "    \n",
    "    lines.extend(sparkline)\n",
    "    left_end = strify(hist_bins[0])\n",
    "    right_end = strify(hist_bins[1])\n",
    "    W = len(sparkline[0]) - len(left_end) - len(right_end)\n",
    "    \n",
    "    if W > 0:\n",
    "        lines.append(f\"{left_end}{'-'*W}{right_end}\")\n",
    "    else:\n",
    "        lines.append(f\"o {left_end} (left endpoint)\")\n",
    "        lines.append(f\"{'-'*(len(sparkline[0])-1)}o {right_end} (right endpoint)\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def summarize_times(arr: list[float, timedelta]):\n",
    "    as_seconds = [x / timedelta(seconds=1) for x in arr]\n",
    "    return summarize(as_seconds, strify=lambda x: str(timedelta(seconds=x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d64100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_batch_iteration_speed_and_cost(\n",
    "    batch_size: int,\n",
    "    pyd: Dataset,\n",
    "    n_iter_samples: int = 30,\n",
    "    collate_fn: Callable | None = None,\n",
    "    num_workers: int | None = None,\n",
    "):\n",
    "    def make_dataloader():\n",
    "        dataloader_kwargs = {'dataset': pyd, 'batch_size': batch_size, 'shuffle': True}\n",
    "        if collate_fn is not None:\n",
    "            dataloader_kwargs['collate_fn'] = collate_fn\n",
    "        if num_workers is not None:\n",
    "            dataloader_kwargs['num_workers'] = num_workers\n",
    "        return DataLoader(**dataloader_kwargs)\n",
    "\n",
    "    dataloader = make_dataloader()\n",
    "    batch_sizes = defaultdict(list)\n",
    "    total_sizes = []\n",
    "    for batch in tqdm(dataloader, leave=False):\n",
    "        total_size = 0\n",
    "        for k, v in batch.items():\n",
    "            if v is None: continue\n",
    "            el_size = v.element_size() * v.nelement()\n",
    "            batch_sizes[k].append(el_size)\n",
    "            total_size += el_size\n",
    "        total_sizes.append(total_size)\n",
    "\n",
    "    batch_iteration_times = []\n",
    "    for samp in tqdm(list(range(n_iter_samples)), leave=False, desc=\"Sampling Dataloader Iteration Speed\"):\n",
    "        dataloader = make_dataloader()\n",
    "        st = datetime.now()\n",
    "        for batch in tqdm(dataloader, leave=False, desc=\"Sampling Batch\"):\n",
    "            pass\n",
    "        batch_iteration_times.append((datetime.now() - st) / len(dataloader))\n",
    "\n",
    "    print(\n",
    "        f\"Iterating through an entire dataloader of {len(dataloader)} batches of size {batch_size} \"\n",
    "        f\"took the following time per batch:\\n{summarize_times(batch_iteration_times)}\\n\\n\"\n",
    "        f\"Total batch size:\\n{summarize(total_sizes)}\"\n",
    "    )\n",
    "    for k, v in batch_sizes.items():\n",
    "        print(f\"  Size of {k}:\\n    {summarize(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d054bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 832.78 MiB, increment: 322.12 MiB\n",
      "CPU times: user 2.08 s, sys: 214 ms, total: 2.3 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "pyd_config = PytorchDatasetConfig(\n",
    "    save_dir=ESD.config.save_dir,\n",
    "    max_seq_len=1024,\n",
    ")\n",
    "pyd = PytorchDataset(config=pyd_config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c71a555",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 5 batches of size 16 took the following time per batch:\n",
      "0:00:00.745001 ± 0:00:00.085648 (0:00:00.588025-0:00:00.939725)\n",
      "Histogram:\n",
      "▄▂▁▄█▃▃▁▂▂\n",
      "o 0:00:00.588025 (left endpoint)\n",
      "---------o 0:00:00.623195 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "8.3 MB ± 1.2 MB (6.3 MB-9.7 MB)\n",
      "  Size of event_mask:\n",
      "    16.4 kB ± 0 Bytes (16.4 kB-16.4 kB)\n",
      "  Size of time_delta:\n",
      "    65.5 kB ± 0 Bytes (65.5 kB-65.5 kB)\n",
      "  Size of static_indices:\n",
      "    128 Bytes ± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of static_measurement_indices:\n",
      "    128 Bytes ± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of dynamic_indices:\n",
      "    3.1 MB ± 468.9 kB (2.4 MB-3.7 MB)\n",
      "  Size of dynamic_measurement_indices:\n",
      "    3.1 MB ± 468.9 kB (2.4 MB-3.7 MB)\n",
      "  Size of dynamic_values:\n",
      "    1.6 MB ± 234.5 kB (1.2 MB-1.8 MB)\n",
      "  Size of dynamic_values_mask:\n",
      "    393.2 kB ± 58.6 kB (294.9 kB-458.8 kB)\n"
     ]
    }
   ],
   "source": [
    "profile_batch_iteration_speed_and_cost(\n",
    "    batch_size=16, pyd=pyd, n_iter_samples=30, collate_fn=pyd.collate,\n",
    "    num_workers=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd999ce",
   "metadata": {},
   "source": [
    "## Other Pipelines\n",
    "### TemporAI Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a013487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d978222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_to_temporai(ESD: Dataset) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Converts an ESD data format into a TemporAI dataset format.\"\"\"\n",
    "\n",
    "    static_df = (\n",
    "        ESD.subjects_df\n",
    "        .select(\n",
    "            'subject_id',\n",
    "            *[pl.col(c) for c, cfg in ESD.measurement_configs.items() if cfg.temporality == 'static']\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(\"subject_id\")\n",
    "    )\n",
    "    \n",
    "    # For the time-series dataframe, as they need only one row per subject ID, timestamp, we need to use the wide\n",
    "    # format of the flat representation. \n",
    "    \n",
    "    flat_reps_dir = ESD.config.save_dir / \"flat_reps\" / \"raw\"\n",
    "    if not flat_reps_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Must have pre-cached flat representations at {flat_reps_dir}!\")\n",
    "        \n",
    "    time_series_df = (\n",
    "        pl.scan_parquet(flat_reps_dir / \"*\" / \"*.parquet\")\n",
    "        .select(\"subject_id\", \"timestamp\", cs.starts_with(\"dynamic\"))\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "        .set_index([\"subject_id\", \"timestamp\"])\n",
    "    )\n",
    "    \n",
    "    return static_df, time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67048d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31867b3031347ea88a2c2ca35ba4941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening Splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 827.43 MiB, increment: 0.40 MiB\n",
      "CPU times: user 481 ms, sys: 40.2 ms, total: 521 ms\n",
      "Wall time: 520 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# We need to convert to a flat format prior to getting temporai representations.\n",
    "# The performance #s here are not reliable as these files may be already generated.\n",
    "ESD.cache_flat_representation(\n",
    "    subjects_per_output_file=None,\n",
    "    feature_inclusion_frequency=None,\n",
    "    do_overwrite=False,\n",
    "    do_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bde3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1893.62 MiB, increment: 1066.19 MiB\n",
      "CPU times: user 1.37 s, sys: 569 ms, total: 1.94 s\n",
      "Wall time: 927 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static, temporai_ts = ESD_to_temporai(ESD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76f6cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporAI uses two dataframes, a static dataframe of shape (100, 1) and a time series dataframe of shape (530742, 160).\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"TemporAI uses two dataframes, a static dataframe of shape {temporai_static.shape} \"\n",
    "    f\"and a time series dataframe of shape {temporai_ts.shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7346bbe",
   "metadata": {},
   "source": [
    "Let's save these dataframes to disk, so we can inspect their disk cost and the memory cost to re-load them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41127e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compressed data takes up 23.9 MB on disk.\n",
      "The uncompressed data takes up 26.0 MB on disk (this is a good approximation of memory cost as it is uncompressed).\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"./speed_comparisons/temporai/compressed\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts.to_parquet(save_dir / \"ts.parquet\")\n",
    "\n",
    "uncompressed_save_dir = Path(\"./speed_comparisons/temporai/uncompressed\")\n",
    "uncompressed_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(uncompressed_save_dir / \"static.parquet\", compression=None)\n",
    "temporai_ts.to_parquet(uncompressed_save_dir / \"ts.parquet\", compression=None)\n",
    "\n",
    "compressed_temporai_size = sum(f.stat().st_size for f in save_dir.glob('**/*') if f.is_file())\n",
    "uncompressed_temporai_size = sum(f.stat().st_size for f in uncompressed_save_dir.glob('**/*') if f.is_file())\n",
    "\n",
    "print(\n",
    "    f\"The compressed data takes up {naturalsize(compressed_temporai_size)} on disk.\\n\"\n",
    "    f\"The uncompressed data takes up {naturalsize(uncompressed_temporai_size)} on disk \"\n",
    "    \"(this is a good approximation of memory cost as it is uncompressed).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a5fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2164.85 MiB, increment: 647.78 MiB\n",
      "CPU times: user 969 ms, sys: 406 ms, total: 1.37 s\n",
      "Wall time: 830 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static = pd.read_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts = pd.read_parquet(save_dir / \"ts.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c13a4a",
   "metadata": {},
   "source": [
    "TemporAI generally converts their timeseries data into a dense, 3D matrix across samples, timepoints, and features. For use in ML pipelines, this is then generally iterated through directly via simple numpy iteration. \n",
    "\n",
    "For example: \n",
    "  * Datasets are converted to 3D views here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/plugins/prediction/one_off/classification/__init__.py#L59 and https://github.com/vanderschaarlab/temporai/blob/67ebd74dc24728163d9aec37f1771a83fc3346e2/src/tempor/data/utils.py#L49\n",
    "  * Iteration through numpy arrays happens here: https://github.com/vanderschaarlab/temporai/blob/main/src/tempor/models/ddh.py#L155\n",
    "  \n",
    "Though a full comparison warrants use of their library (and will further depend on the exact model used (as each has different strategies for processing data), we can simulate that approach here quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8bb253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_categories(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[c]):\n",
    "            df[c] = df[c].cat.codes\n",
    "    return df\n",
    "\n",
    "def to_3D_arr(df: pd.DataFrame, max_timesteps: int | None = None) -> np.ndarray:\n",
    "    df = no_categories(df)\n",
    "    samples = set(df.index.get_level_values(0))\n",
    "    num_samples = len(samples)\n",
    "    num_features = len(df.columns)\n",
    "    num_timesteps_per_sample = df.groupby(level=0).size()\n",
    "    max_actual_timesteps = num_timesteps_per_sample.max()\n",
    "    max_timesteps = max_actual_timesteps if max_timesteps is None else max_timesteps\n",
    "    array = np.full(shape=(num_samples, max_timesteps, num_features), fill_value=np.NaN)\n",
    "    for i_sample, idx_sample in enumerate(samples):\n",
    "        set_vals = df.loc[idx_sample, :, :].to_numpy()[:max_timesteps, :]  # pyright: ignore\n",
    "        if i_sample == 0:\n",
    "            array = array.astype(set_vals.dtype)  # Need to cast to the type matching source data.\n",
    "        array[i_sample, : num_timesteps_per_sample[idx_sample], :] = set_vals  # pyright: ignore\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "367b17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTemporAIStyleDataset(Dataset):\n",
    "    def __init__(self, static: np.ndarray, ts: np.ndarray):\n",
    "        self.static = static\n",
    "        self.ts = ts\n",
    "        \n",
    "    def __len__(self) -> int: return self.ts.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict[str, torch.Tensor]:\n",
    "        return {'static': torch.Tensor(self.static[idx]), 'ts': torch.Tensor(self.ts[idx])}\n",
    "    \n",
    "def profile_temporai_dataset(\n",
    "    temporai_static, temporai_ts, batch_size: int = 16,\n",
    "    n_iter_samples: int = 30,\n",
    "    max_seq_len: int = 32,\n",
    "):\n",
    "    static_as_np = np.nan_to_num(no_categories(temporai_static).to_numpy(), nan=0)\n",
    "    ts_as_np = np.nan_to_num(to_3D_arr(temporai_ts, max_timesteps=max_seq_len), nan=0)\n",
    "    print(\n",
    "        f\"Yielded a static NP array of shape {static_as_np.shape} and a TS NP array \"\n",
    "        f\"of shape {ts_as_np.shape}.\"\n",
    "    )\n",
    "    temporai_pyd = SimpleTemporAIStyleDataset(static_as_np, ts_as_np)\n",
    "\n",
    "    profile_batch_iteration_speed_and_cost(\n",
    "        batch_size=batch_size, pyd=temporai_pyd, n_iter_samples=n_iter_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba4786df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yielded a static NP array of shape (100, 1) and a TS NP array of shape (100, 1024, 160).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 7 batches of size 16 took the following time per batch:\n",
      "0:00:00.007341 ± 0:00:00.001583 (0:00:00.006151-0:00:00.013234)\n",
      "Histogram:\n",
      "█▄▂▁▁▁▁▁▁▁\n",
      "o 0:00:00.006151 (left endpoint)\n",
      "---------o 0:00:00.006859 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "9.4 MB ± 2.8 MB (2.6 MB-10.5 MB)\n",
      "  Size of static:\n",
      "    57 Bytes ± 16 Bytes (16 Bytes-64 Bytes)\n",
      "  Size of ts:\n",
      "    9.4 MB ± 2.8 MB (2.6 MB-10.5 MB)\n",
      "peak memory: 2276.14 MiB, increment: 342.82 MiB\n",
      "CPU times: user 7.1 s, sys: 317 ms, total: 7.41 s\n",
      "Wall time: 2.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "profile_temporai_dataset(temporai_static, temporai_ts, batch_size=16, n_iter_samples=30, max_seq_len=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906d5aa",
   "metadata": {},
   "source": [
    "As we can see, the strategy of featurizing and batching used in TemporAI results (on this synthetic dataset) in a significantly faster iteration speed and a marginally lower memory cost than does the strategy used in ESGPT (all formats are mean ± standard deviation (min - max)\n",
    "\n",
    "TemporAI Speed: `0:00:00.007319 ± 0:00:00.001439 (0:00:00.006224-0:00:00.012835)`  \n",
    "ESGPT Speed:    `0:00:00.734263 ± 0:00:00.078552 (0:00:00.586128-0:00:00.871448)`\n",
    "\n",
    "TemporAI Memory: `9.4 MB ± 2.8 MB (2.6 MB-10.5 MB)`  \n",
    "ESGPT Memory:    `8.3 MB ± 1.1 MB (6.6 MB-9.7 MB)`\n",
    "\n",
    "In table form (using chatGPT for conversions, so may need to be double checked), where \"Delta\" means what % of TemporAI's resource cost does ESGPT _save_ (higher is better), we get the following:\n",
    "|                      | TemporAI              | ESGPT                 | Delta (%)  |\n",
    "|----------------------|-----------------------|-----------------------|------------|\n",
    "| **Iteration time / batch (ms)** | 7.32 ± 1.44 (6.22 - 12.8) | 734 ± 78.6 (586 - 871) | -9943%           |\n",
    "| **Memory (MB)**      | 9.4 ± 2.8 (2.6 - 10.5) | 8.3 ± 1.1 (6.6 - 9.7) | 11.7%             |\n",
    "\n",
    "There are some biases in this format, on both sides:\n",
    "  1. ESGPT samples different subsequences per item iteration, whereas TemporAI is limited to only using the first max subsequence samples. \n",
    "  2. This dataset has relatively few measurements, which will reduce the memory disparity between the two formats (this bias favors TemporAI).\n",
    "  3. The strategy of flattening this dataset may induce too much memory overhead, as if multiple measurements are not common within an event, it will have extra columns that TemporAI does not need. Conversely, it may reduce a significant amount of data, as if there are many measurements than a simple count, sum, sum_sqd, min, and max representation will not fully capture the data, thereby reducing the burden on TemporAI. (This bias could favor either).\n",
    "  \n",
    "#### On MIMIC-IV\n",
    "Ultimately, these numbers will only be truly reasonable when compared on real data. To do so, we can use MIMIC-IV. While the full numbers for the MIMIC-IV dataset can be found in that example's repository, here we summarize the table obtained when running on that dataset with an otherwise nearly identical setup:\n",
    "\n",
    "|                      | TemporAI             | ESGPT                | Improvement (%)  |\n",
    "|----------------------|----------------------|----------------------|-------------------|\n",
    "| **Iteration time / batch (ms)**       | 597 ± 20.6 (571 - 624) | 416 ± 4.16 (410 - 423) | 30.3%            |\n",
    "| **Memory (MB)**      | 877 ± 95.5 (507 - 902) | 67.2 ± 35.0 (18.3 - 411) | 92.3%           |\n",
    "\n",
    "We can see that the numbers here are very different, favoring ESGPT dramatically more than we saw on sample data. The reasons for this are primarily due to how memory intensive TemporAI's coding system is; whereas ESGPT only stores data-elements in a batch if those elements were actually observed in the record, TemporAI's flat encoding means that batches must have a memory cost that scales with the total vocabulary size (and overall maximum sequence length) of the data. This higher cost impacts both speed and memory, though memory is the clear focus for ESGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfc953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
