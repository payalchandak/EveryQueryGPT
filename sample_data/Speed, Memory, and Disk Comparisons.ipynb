{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652c54c0",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Disk Comparisons\n",
    "\n",
    "In this notebook, we'll offer some rough comparisons of the computational performance implications of ESGPT vs. other competing pipelines. We'll focus these comparisons on several metrics:\n",
    "  1. The time, runtime memory, and final disk space required to construct, pre-process, and store an ESGPT dataset relative to other pipelines, where applicable.\n",
    "  2. The initialization time, iteration speed, and GPU memory costs for producing batches of data within the ESGPT framework vs. other systems.\n",
    "  \n",
    "In particular, we'll compare (or justify why they are inappropriate comparators) against the following pipelines:\n",
    "  1. TemporAI\n",
    "  2. OMOP-Learn\n",
    "  3. FIDDLE\n",
    "  4. MIMIC-Extract\n",
    "  \n",
    "We'll make these comparisons leveraging the synthetic data distributed with ESGPT's sample tutorial, but this code can also be ported to any other dataset to run these profiles locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23533dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from humanize import naturalsize, naturaldelta\n",
    "from pathlib import Path\n",
    "from sparklines import sparklines\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "from EventStream.data.config import PytorchDatasetConfig\n",
    "from EventStream.data.types import PytorchBatch\n",
    "from EventStream.data.pytorch_dataset import PytorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87fd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6314e23",
   "metadata": {},
   "source": [
    "First, let's check and see how much disk space the dataset uses, and in what components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset takes up 164.5 MB on disk, which includes:\n",
      "  * 19.5 MB for the core dataset.\n",
      "  * 11.7 MB for the deep-learning representation dataframes.\n",
      "  * 133.2 MB for the flat representation dataframes.\n"
     ]
    }
   ],
   "source": [
    "total_dataset_size = sum(f.stat().st_size for f in dataset_dir.glob('**/*') if f.is_file())\n",
    "DL_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"DL_reps\").glob('**/*') if f.is_file())\n",
    "just_dataset_size = total_dataset_size - DL_reps_size\n",
    "\n",
    "if (dataset_dir / \"flat_reps\").is_dir():\n",
    "    flat_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"flat_reps\").glob('**/*') if f.is_file())\n",
    "    just_dataset_size -= flat_reps_size\n",
    "    flat_reps_lines = [f\"  * {naturalsize(flat_reps_size)} for the flat representation dataframes.\"]\n",
    "else:\n",
    "    flat_reps_lines = []\n",
    "\n",
    "lines = [\n",
    "    f\"The total dataset takes up {naturalsize(total_dataset_size)} on disk, which includes:\",\n",
    "    f\"  * {naturalsize(just_dataset_size)} for the core dataset.\",\n",
    "    f\"  * {naturalsize(DL_reps_size)} for the deep-learning representation dataframes.\",\n",
    "] + flat_reps_lines\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d866",
   "metadata": {},
   "source": [
    "First, we'll note that loading a dataset doesn't require much of either resource. This is because the data is loaded lazily, so complex dataframe elements aren't loaded until they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 347.95 MiB, increment: 1.88 MiB\n",
      "CPU times: user 126 ms, sys: 22.8 ms, total: 149 ms\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92467145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n",
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n",
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n",
      "peak memory: 507.10 MiB, increment: 158.85 MiB\n",
      "CPU times: user 376 ms, sys: 133 ms, total: 509 ms\n",
      "Wall time: 339 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "s_df = ESD.subjects_df\n",
    "e_df = ESD.events_df\n",
    "dm_df = ESD.dynamic_measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee9d9d",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Stats\n",
    "Now let's load a pytorch dataset and examine iteration speed and GPU memory cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr: list[float], strify: Callable[float, str] = naturalsize) -> str:\n",
    "    mean, std, mn, mx = np.mean(arr), np.std(arr), np.min(arr), np.max(arr)\n",
    "    simple_summ = f\"{strify(mean)} Â± {strify(std)} ({strify(mn)}-{strify(mx)})\"\n",
    "    \n",
    "    if len(arr) < 25: return simple_summ\n",
    "    \n",
    "    hist_vals, hist_bins = np.histogram(arr)\n",
    "    lines = [simple_summ, \"Histogram:\"]\n",
    "    sparkline = sparklines(hist_vals)\n",
    "    \n",
    "    lines.extend(sparkline)\n",
    "    left_end = strify(hist_bins[0])\n",
    "    right_end = strify(hist_bins[1])\n",
    "    W = len(sparkline[0]) - len(left_end) - len(right_end)\n",
    "    \n",
    "    if W > 0:\n",
    "        lines.append(f\"{left_end}{'-'*W}{right_end}\")\n",
    "    else:\n",
    "        lines.append(f\"o {left_end} (left endpoint)\")\n",
    "        lines.append(f\"{'-'*(len(sparkline[0])-1)}o {right_end} (right endpoint)\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def summarize_times(arr: list[float, timedelta]):\n",
    "    as_seconds = [x / timedelta(seconds=1) for x in arr]\n",
    "    return summarize(as_seconds, strify=lambda x: str(timedelta(seconds=x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d054bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 836.55 MiB, increment: 329.29 MiB\n",
      "CPU times: user 2.12 s, sys: 191 ms, total: 2.31 s\n",
      "Wall time: 2.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "pyd_config = PytorchDatasetConfig(\n",
    "    save_dir=ESD.config.save_dir,\n",
    "    max_seq_len=32,\n",
    ")\n",
    "pyd = PytorchDataset(config=pyd_config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c71a555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 5 batches of size 16 took the following time per batch:\n",
      "0:00:00.029526 Â± 0:00:00.002528 (0:00:00.026819-0:00:00.037488)\n",
      "Histogram:\n",
      "ââââââââââ\n",
      "o 0:00:00.026819 (left endpoint)\n",
      "---------o 0:00:00.027886 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "200.7 kB Â± 40.6 kB (142.6 kB-250.1 kB)\n",
      "  Size of event_mask:\n",
      "    512 Bytes Â± 0 Bytes (512 Bytes-512 Bytes)\n",
      "  Size of time_delta:\n",
      "    2.0 kB Â± 0 Bytes (2.0 kB-2.0 kB)\n",
      "  Size of static_indices:\n",
      "    128 Bytes Â± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of static_measurement_indices:\n",
      "    128 Bytes Â± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of dynamic_indices:\n",
      "    75.4 kB Â± 15.5 kB (53.2 kB-94.2 kB)\n",
      "  Size of dynamic_measurement_indices:\n",
      "    75.4 kB Â± 15.5 kB (53.2 kB-94.2 kB)\n",
      "  Size of dynamic_values:\n",
      "    37.7 kB Â± 7.7 kB (26.6 kB-47.1 kB)\n",
      "  Size of dynamic_values_mask:\n",
      "    9.4 kB Â± 1.9 kB (6.7 kB-11.8 kB)\n",
      "peak memory: 810.73 MiB, increment: 12.79 MiB\n",
      "CPU times: user 5.73 s, sys: 221 ms, total: 5.95 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "batch_size = 16\n",
    "n_iter_samples = 30\n",
    "\n",
    "dataloader = DataLoader(pyd, collate_fn=pyd.collate, batch_size=batch_size)\n",
    "\n",
    "batch_sizes = defaultdict(list)\n",
    "total_sizes = []\n",
    "for batch in tqdm(dataloader, leave=False):\n",
    "    total_size = 0\n",
    "    for k, v in batch.items():\n",
    "        if v is None: continue\n",
    "        el_size = v.element_size() * v.nelement()\n",
    "        batch_sizes[k].append(el_size)\n",
    "        total_size += el_size\n",
    "    total_sizes.append(total_size)\n",
    "\n",
    "batch_iteration_times = []\n",
    "for samp in tqdm(list(range(n_iter_samples)), leave=False, desc=\"Sampling Dataloader Iteration Speed\"):\n",
    "    dataloader = DataLoader(pyd, collate_fn=pyd.collate, batch_size=batch_size, shuffle=True)\n",
    "    st = datetime.now()\n",
    "    for batch in tqdm(dataloader, leave=False, desc=\"Sampling Batch\"):\n",
    "        pass\n",
    "    batch_iteration_times.append((datetime.now() - st) / len(dataloader))\n",
    "    \n",
    "print(\n",
    "    f\"Iterating through an entire dataloader of {len(dataloader)} batches of size {batch_size} \"\n",
    "    f\"took the following time per batch:\\n{summarize_times(batch_iteration_times)}\\n\\n\"\n",
    "    f\"Total batch size:\\n{summarize(total_sizes)}\"\n",
    ")\n",
    "for k, v in batch_sizes.items():\n",
    "    print(f\"  Size of {k}:\\n    {summarize(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd999ce",
   "metadata": {},
   "source": [
    "## Other Pipelines\n",
    "### TemporAI Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a013487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d978222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_to_temporai(ESD: Dataset) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Converts an ESD data format into a TemporAI dataset format.\"\"\"\n",
    "\n",
    "    static_df = (\n",
    "        ESD.subjects_df\n",
    "        .select(\n",
    "            'subject_id',\n",
    "            *[pl.col(c) for c, cfg in ESD.measurement_configs.items() if cfg.temporality == 'static']\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(\"subject_id\")\n",
    "    )\n",
    "    \n",
    "    # For the time-series dataframe, as they need only one row per subject ID, timestamp, we need to use the wide\n",
    "    # format of the flat representation. \n",
    "    \n",
    "    flat_reps_dir = ESD.config.save_dir / \"flat_reps\" / \"raw\"\n",
    "    if not flat_reps_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Must have pre-cached flat representations at {flat_reps_dir}!\")\n",
    "        \n",
    "    time_series_df = (\n",
    "        pl.scan_parquet(flat_reps_dir / \"*\" / \"*.parquet\")\n",
    "        .select(\"subject_id\", \"timestamp\", cs.starts_with(\"dynamic\"))\n",
    "        .collect()\n",
    "        .to_pandas()\n",
    "        .set_index([\"subject_id\", \"timestamp\"])\n",
    "    )\n",
    "    \n",
    "    return static_df, time_series_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67048d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da20a3a256e4144adc8484f84b7a369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening Splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Subject chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1972.99 MiB, increment: 0.10 MiB\n",
      "CPU times: user 319 ms, sys: 75.8 ms, total: 395 ms\n",
      "Wall time: 529 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# We need to convert to a flat format prior to getting temporai representations.\n",
    "# The performance #s here are not reliable as these files may be already generated.\n",
    "ESD.cache_flat_representation(\n",
    "    subjects_per_output_file=None,\n",
    "    feature_inclusion_frequency=None,\n",
    "    do_overwrite=False,\n",
    "    do_update=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bde3ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2439.01 MiB, increment: 963.00 MiB\n",
      "CPU times: user 1.46 s, sys: 912 ms, total: 2.37 s\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static, temporai_ts = ESD_to_temporai(ESD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f6cf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporAI uses two dataframes, a static dataframe of shape (100, 1) and a time series dataframe of shape (530742, 160).\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"TemporAI uses two dataframes, a static dataframe of shape {temporai_static.shape} \"\n",
    "    f\"and a time series dataframe of shape {temporai_ts.shape}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7346bbe",
   "metadata": {},
   "source": [
    "Let's save these dataframes to disk, so we can inspect their disk cost and the memory cost to re-load them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41127e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compressed data takes up 23.9 MB on disk.\n",
      "The uncompressed data takes up 26.0 MB on disk (this is a good approximation of memory cost as it is uncompressed).\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path(\"./speed_comparisons/temporai/compressed\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts.to_parquet(save_dir / \"ts.parquet\")\n",
    "\n",
    "uncompressed_save_dir = Path(\"./speed_comparisons/temporai/uncompressed\")\n",
    "uncompressed_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temporai_static.to_parquet(uncompressed_save_dir / \"static.parquet\", compression=None)\n",
    "temporai_ts.to_parquet(uncompressed_save_dir / \"ts.parquet\", compression=None)\n",
    "\n",
    "compressed_temporai_size = sum(f.stat().st_size for f in save_dir.glob('**/*') if f.is_file())\n",
    "uncompressed_temporai_size = sum(f.stat().st_size for f in uncompressed_save_dir.glob('**/*') if f.is_file())\n",
    "\n",
    "print(\n",
    "    f\"The compressed data takes up {naturalsize(compressed_temporai_size)} on disk.\\n\"\n",
    "    f\"The uncompressed data takes up {naturalsize(uncompressed_temporai_size)} on disk \"\n",
    "    \"(this is a good approximation of memory cost as it is uncompressed).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4a5fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2948.04 MiB, increment: 929.04 MiB\n",
      "CPU times: user 1.11 s, sys: 1.06 s, total: 2.17 s\n",
      "Wall time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "temporai_static = pd.read_parquet(save_dir / \"static.parquet\")\n",
    "temporai_ts = pd.read_parquet(save_dir / \"ts.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83343c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
