{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652c54c0",
   "metadata": {},
   "source": [
    "# Speed, Memory, and Disk Comparisons\n",
    "\n",
    "In this notebook, we'll offer some rough comparisons of the computational performance implications of ESGPT vs. other competing pipelines. We'll focus these comparisons on several metrics:\n",
    "  1. The time, runtime memory, and final disk space required to construct, pre-process, and store an ESGPT dataset relative to other pipelines, where applicable.\n",
    "  2. The initialization time, iteration speed, and GPU memory costs for producing batches of data within the ESGPT framework vs. other systems.\n",
    "  \n",
    "In particular, we'll compare (or justify why they are inappropriate comparators) against the following pipelines:\n",
    "  1. TemporAI\n",
    "  2. OMOP-Learn\n",
    "  3. FIDDLE\n",
    "  4. MIMIC-Extract\n",
    "  \n",
    "We'll make these comparisons leveraging the synthetic data distributed with ESGPT's sample tutorial, but this code can also be ported to any other dataset to run these profiles locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dac655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23533dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from humanize import naturalsize, naturaldelta\n",
    "from pathlib import Path\n",
    "from sparklines import sparklines\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Callable\n",
    "\n",
    "from EventStream.data.dataset_polars import Dataset\n",
    "from EventStream.data.config import PytorchDatasetConfig\n",
    "from EventStream.data.types import PytorchBatch\n",
    "from EventStream.data.pytorch_dataset import PytorchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b87fd07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path(os.getcwd()) / \"processed/sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6314e23",
   "metadata": {},
   "source": [
    "First, let's check and see how much disk space the dataset uses, and in what components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3490e188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total dataset takes up 31.2 MB on disk, which includes:\n",
      "  * 19.5 MB for the core dataset.\n",
      "  * 11.7 MB for the deep-learning representation dataframes.\n"
     ]
    }
   ],
   "source": [
    "total_dataset_size = sum(f.stat().st_size for f in dataset_dir.glob('**/*') if f.is_file())\n",
    "DL_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"DL_reps\").glob('**/*') if f.is_file())\n",
    "just_dataset_size = total_dataset_size - DL_reps_size\n",
    "\n",
    "if (dataset_dir / \"flat_reps\").is_dir():\n",
    "    flat_reps_size = sum(f.stat().st_size for f in (dataset_dir / \"flat_reps\").glob('**/*') if f.is_file())\n",
    "    just_dataset_size -= flat_reps_size\n",
    "    flat_reps_lines = [f\"  * {naturalsize(flat_reps_size)} for the flat representation dataframes.\"]\n",
    "else:\n",
    "    flat_reps_lines = []\n",
    "\n",
    "lines = [\n",
    "    f\"The total dataset takes up {naturalsize(total_dataset_size)} on disk, which includes:\",\n",
    "    f\"  * {naturalsize(just_dataset_size)} for the core dataset.\",\n",
    "    f\"  * {naturalsize(DL_reps_size)} for the deep-learning representation dataframes.\",\n",
    "] + flat_reps_lines\n",
    "\n",
    "print('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62d866",
   "metadata": {},
   "source": [
    "First, we'll note that loading a dataset doesn't require much of either resource. This is because the data is loaded lazily, so complex dataframe elements aren't loaded until they are needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bb0205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 346.61 MiB, increment: 1.75 MiB\n",
      "CPU times: user 133 ms, sys: 32.2 ms, total: 165 ms\n",
      "Wall time: 277 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "ESD = Dataset.load(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92467145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subjects from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/subjects_df.parquet...\n",
      "Loading events from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/events_df.parquet...\n",
      "Loading dynamic_measurements from /home/mmd/Projects/EventStreamGPT/sample_data/processed/sample/dynamic_measurements_df.parquet...\n",
      "peak memory: 505.35 MiB, increment: 158.61 MiB\n",
      "CPU times: user 348 ms, sys: 123 ms, total: 472 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "s_df = ESD.subjects_df\n",
    "e_df = ESD.events_df\n",
    "dm_df = ESD.dynamic_measurements_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee9d9d",
   "metadata": {},
   "source": [
    "## Pytorch Dataset Stats\n",
    "Now let's load a pytorch dataset and examine iteration speed and GPU memory cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61e8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(arr: list[float], strify: Callable[float, str] = naturalsize) -> str:\n",
    "    mean, std, mn, mx = np.mean(arr), np.std(arr), np.min(arr), np.max(arr)\n",
    "    simple_summ = f\"{strify(mean)} ± {strify(std)} ({strify(mn)}-{strify(mx)})\"\n",
    "    \n",
    "    if len(arr) < 25: return simple_summ\n",
    "    \n",
    "    hist_vals, hist_bins = np.histogram(arr)\n",
    "    lines = [simple_summ, \"Histogram:\"]\n",
    "    sparkline = sparklines(hist_vals)\n",
    "    \n",
    "    lines.extend(sparkline)\n",
    "    left_end = strify(hist_bins[0])\n",
    "    right_end = strify(hist_bins[1])\n",
    "    W = len(sparkline[0]) - len(left_end) - len(right_end)\n",
    "    \n",
    "    if W > 0:\n",
    "        lines.append(f\"{left_end}{'-'*W}{right_end}\")\n",
    "    else:\n",
    "        lines.append(f\"o {left_end} (left endpoint)\")\n",
    "        lines.append(f\"{'-'*(len(sparkline[0])-1)}o {right_end} (right endpoint)\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def summarize_times(arr: list[float, timedelta]):\n",
    "    as_seconds = [x / timedelta(seconds=1) for x in arr]\n",
    "    return summarize(as_seconds, strify=lambda x: str(timedelta(seconds=x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d054bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 817.68 MiB, increment: 312.33 MiB\n",
      "CPU times: user 2.04 s, sys: 148 ms, total: 2.18 s\n",
      "Wall time: 2.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "pyd_config = PytorchDatasetConfig(\n",
    "    save_dir=ESD.config.save_dir,\n",
    "    max_seq_len=32,\n",
    ")\n",
    "pyd = PytorchDataset(config=pyd_config, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c71a555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Dataloader Iteration Speed:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling Batch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating through an entire dataloader of 5 batches of size 16 took the following time per batch:\n",
      "0:00:00.029285 ± 0:00:00.001813 (0:00:00.027680-0:00:00.035112)\n",
      "Histogram:\n",
      "▇█▅▁▂▂▁▁▁▂\n",
      "o 0:00:00.027680 (left endpoint)\n",
      "---------o 0:00:00.028423 (right endpoint)\n",
      "\n",
      "Total batch size:\n",
      "170.5 kB ± 52.1 kB (110.3 kB-239.4 kB)\n",
      "  Size of event_mask:\n",
      "    512 Bytes ± 0 Bytes (512 Bytes-512 Bytes)\n",
      "  Size of time_delta:\n",
      "    2.0 kB ± 0 Bytes (2.0 kB-2.0 kB)\n",
      "  Size of static_indices:\n",
      "    128 Bytes ± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of static_measurement_indices:\n",
      "    128 Bytes ± 0 Bytes (128 Bytes-128 Bytes)\n",
      "  Size of dynamic_indices:\n",
      "    63.9 kB ± 19.8 kB (41.0 kB-90.1 kB)\n",
      "  Size of dynamic_measurement_indices:\n",
      "    63.9 kB ± 19.8 kB (41.0 kB-90.1 kB)\n",
      "  Size of dynamic_values:\n",
      "    31.9 kB ± 9.9 kB (20.5 kB-45.1 kB)\n",
      "  Size of dynamic_values_mask:\n",
      "    8.0 kB ± 2.5 kB (5.1 kB-11.3 kB)\n",
      "peak memory: 824.50 MiB, increment: 3.96 MiB\n",
      "CPU times: user 5.72 s, sys: 240 ms, total: 5.96 s\n",
      "Wall time: 5.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "batch_size = 16\n",
    "n_iter_samples = 30\n",
    "\n",
    "dataloader = DataLoader(pyd, collate_fn=pyd.collate, batch_size=batch_size)\n",
    "\n",
    "batch_sizes = defaultdict(list)\n",
    "total_sizes = []\n",
    "for batch in tqdm(dataloader, leave=False):\n",
    "    total_size = 0\n",
    "    for k, v in batch.items():\n",
    "        if v is None: continue\n",
    "        el_size = v.element_size() * v.nelement()\n",
    "        batch_sizes[k].append(el_size)\n",
    "        total_size += el_size\n",
    "    total_sizes.append(total_size)\n",
    "\n",
    "batch_iteration_times = []\n",
    "for samp in tqdm(list(range(n_iter_samples)), leave=False, desc=\"Sampling Dataloader Iteration Speed\"):\n",
    "    dataloader = DataLoader(pyd, collate_fn=pyd.collate, batch_size=batch_size, shuffle=True)\n",
    "    st = datetime.now()\n",
    "    for batch in tqdm(dataloader, leave=False, desc=\"Sampling Batch\"):\n",
    "        pass\n",
    "    batch_iteration_times.append((datetime.now() - st) / len(dataloader))\n",
    "    \n",
    "print(\n",
    "    f\"Iterating through an entire dataloader of {len(dataloader)} batches of size {batch_size} \"\n",
    "    f\"took the following time per batch:\\n{summarize_times(batch_iteration_times)}\\n\\n\"\n",
    "    f\"Total batch size:\\n{summarize(total_sizes)}\"\n",
    ")\n",
    "for k, v in batch_sizes.items():\n",
    "    print(f\"  Size of {k}:\\n    {summarize(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd999ce",
   "metadata": {},
   "source": [
    "## Other Pipelines\n",
    "### TemporAI Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a013487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9871c415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (530_742, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>event_id</th><th>subject_id</th><th>timestamp</th><th>event_type</th><th>age</th><th>age_is_inlier</th></tr><tr><td>u32</td><td>u8</td><td>datetime[μs]</td><td>cat</td><td>f64</td><td>bool</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>2010-06-24 13:23:00</td><td>&quot;ADMISSION&amp;VITA…</td><td>-0.558276</td><td>true</td></tr><tr><td>1</td><td>0</td><td>2010-06-24 14:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.55825</td><td>true</td></tr><tr><td>2</td><td>0</td><td>2010-06-24 15:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558224</td><td>true</td></tr><tr><td>3</td><td>0</td><td>2010-06-24 16:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558199</td><td>true</td></tr><tr><td>4</td><td>0</td><td>2010-06-24 17:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558173</td><td>true</td></tr><tr><td>5</td><td>0</td><td>2010-06-24 18:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558148</td><td>true</td></tr><tr><td>6</td><td>0</td><td>2010-06-24 19:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558122</td><td>true</td></tr><tr><td>7</td><td>0</td><td>2010-06-24 20:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.558097</td><td>true</td></tr><tr><td>8</td><td>0</td><td>2010-06-24 21:23:00</td><td>&quot;LAB&quot;</td><td>-0.558071</td><td>true</td></tr><tr><td>9</td><td>0</td><td>2010-06-24 22:23:00</td><td>&quot;LAB&quot;</td><td>-0.558045</td><td>true</td></tr><tr><td>10</td><td>0</td><td>2010-06-24 23:23:00</td><td>&quot;LAB&quot;</td><td>-0.55802</td><td>true</td></tr><tr><td>11</td><td>0</td><td>2010-06-25 00:23:00</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-0.557994</td><td>true</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>530730</td><td>99</td><td>2010-11-20 01:20:06</td><td>&quot;LAB&quot;</td><td>-1.088264</td><td>true</td></tr><tr><td>530731</td><td>99</td><td>2010-11-20 02:20:06</td><td>&quot;LAB&quot;</td><td>-1.088239</td><td>true</td></tr><tr><td>530732</td><td>99</td><td>2010-11-20 03:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.088213</td><td>true</td></tr><tr><td>530733</td><td>99</td><td>2010-11-20 04:20:06</td><td>&quot;LAB&quot;</td><td>-1.088187</td><td>true</td></tr><tr><td>530734</td><td>99</td><td>2010-11-20 05:20:06</td><td>&quot;LAB&quot;</td><td>-1.088162</td><td>true</td></tr><tr><td>530735</td><td>99</td><td>2010-11-20 06:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.088136</td><td>true</td></tr><tr><td>530736</td><td>99</td><td>2010-11-20 07:20:06</td><td>&quot;LAB&quot;</td><td>-1.088111</td><td>true</td></tr><tr><td>530737</td><td>99</td><td>2010-11-20 08:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.088085</td><td>true</td></tr><tr><td>530738</td><td>99</td><td>2010-11-20 09:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.08806</td><td>true</td></tr><tr><td>530739</td><td>99</td><td>2010-11-20 10:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.088034</td><td>true</td></tr><tr><td>530740</td><td>99</td><td>2010-11-20 11:20:06</td><td>&quot;VITAL&amp;LAB&quot;</td><td>-1.088008</td><td>true</td></tr><tr><td>530741</td><td>99</td><td>2010-11-20 12:20:06</td><td>&quot;DISCHARGE&amp;LAB&quot;</td><td>-1.087983</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (530_742, 6)\n",
       "┌──────────┬────────────┬─────────────────────┬─────────────────────┬───────────┬───────────────┐\n",
       "│ event_id ┆ subject_id ┆ timestamp           ┆ event_type          ┆ age       ┆ age_is_inlier │\n",
       "│ ---      ┆ ---        ┆ ---                 ┆ ---                 ┆ ---       ┆ ---           │\n",
       "│ u32      ┆ u8         ┆ datetime[μs]        ┆ cat                 ┆ f64       ┆ bool          │\n",
       "╞══════════╪════════════╪═════════════════════╪═════════════════════╪═══════════╪═══════════════╡\n",
       "│ 0        ┆ 0          ┆ 2010-06-24 13:23:00 ┆ ADMISSION&VITAL&LAB ┆ -0.558276 ┆ true          │\n",
       "│ 1        ┆ 0          ┆ 2010-06-24 14:23:00 ┆ VITAL&LAB           ┆ -0.55825  ┆ true          │\n",
       "│ 2        ┆ 0          ┆ 2010-06-24 15:23:00 ┆ VITAL&LAB           ┆ -0.558224 ┆ true          │\n",
       "│ 3        ┆ 0          ┆ 2010-06-24 16:23:00 ┆ VITAL&LAB           ┆ -0.558199 ┆ true          │\n",
       "│ …        ┆ …          ┆ …                   ┆ …                   ┆ …         ┆ …             │\n",
       "│ 530738   ┆ 99         ┆ 2010-11-20 09:20:06 ┆ VITAL&LAB           ┆ -1.08806  ┆ true          │\n",
       "│ 530739   ┆ 99         ┆ 2010-11-20 10:20:06 ┆ VITAL&LAB           ┆ -1.088034 ┆ true          │\n",
       "│ 530740   ┆ 99         ┆ 2010-11-20 11:20:06 ┆ VITAL&LAB           ┆ -1.088008 ┆ true          │\n",
       "│ 530741   ┆ 99         ┆ 2010-11-20 12:20:06 ┆ DISCHARGE&LAB       ┆ -1.087983 ┆ true          │\n",
       "└──────────┴────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────┘"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d978222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ESD_to_temporai(ESD: Dataset) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Converts an ESD data format into a TemporAI dataset format.\"\"\"\n",
    "\n",
    "    static_df = ESD.subjects_df.select(\n",
    "        'subject_id', *[pl.col(c) for c, cfg in ESD.measurement_configs.items() if cfg.temporality == 'static']\n",
    "    ).to_pandas()\n",
    "    static_df.set_index('subject_id', inplace=True)\n",
    "    \n",
    "    time_series_df = (\n",
    "        ESD.events_df\n",
    "        .select(\n",
    "            'subject_id', 'event_id', 'timestamp', *[\n",
    "                pl.col(c) for c, cfg in ESD.measurement_configs.items()\n",
    "                if cfg.temporality == 'functional_time_dependent'\n",
    "            ]\n",
    "        )\n",
    "        .join(\n",
    "            (\n",
    "                ESD.dynamic_measurements_df\n",
    "                .select(\n",
    "                    'event_id', *[\n",
    "                        pl.col(c) for c, cfg in ESD.measurement_configs.items()\n",
    "                        if cfg.temporality == 'dynamic'\n",
    "                    ], *[\n",
    "                        pl.col(cfg.values_column) for _, cfg in ESD.measurement_configs.items()\n",
    "                        if cfg.temporality == 'dynamic' and cfg.modality == 'multivariate_regression'\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "            on=['event_id'],\n",
    "            how='inner'\n",
    "        )\n",
    "        .drop('event_id')\n",
    "        .groupby('subject_id', 'timestamp')\n",
    "        .agg(pl.all().count().map_alias(lambda c: f\"{c}/))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
